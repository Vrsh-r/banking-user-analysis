{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Users"
      ],
      "metadata": {
        "id": "_gIemWSljLT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta"
      ],
      "metadata": {
        "id": "BizWtswujIBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_users=50000\n",
        "user_ids=[f\"U{i:06d}\"for i in range(1,num_users+1)]"
      ],
      "metadata": {
        "id": "EapW8f-3R0Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "devices=np.random.choice(['ios','andriod','web'],size=num_users,p=[0.6,0.3,0.1])"
      ],
      "metadata": {
        "id": "aDkBCn2QUzH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(devices).value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "Zopekv0NV4YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ages=np.random.randint(18,61,size=num_users)"
      ],
      "metadata": {
        "id": "xzQnHuakZGHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_date = datetime(2024, 1, 1)\n",
        "\n",
        "# random days between 0 and 365\n",
        "signup_offsets = np.random.randint(0, 366, size=num_users)\n",
        "\n",
        "signup_dates = [start_date + timedelta(days=int(d)) for d in signup_offsets]"
      ],
      "metadata": {
        "id": "OI8qr6NxfxyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phone_numbers = ['07' + ''.join(np.random.choice(list('0123456789'), size=9)) for _ in range(num_users)]"
      ],
      "metadata": {
        "id": "rOigWvhEgVA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pincodes = np.random.choice(\n",
        "    ['SW1', 'SW3', 'E14', 'W1', 'SE1', 'EC2', 'N1', 'IG1', 'CR0', 'KT1'],\n",
        "    size=num_users\n",
        ")"
      ],
      "metadata": {
        "id": "HLtO17EKgcE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_names = ['Aisha', 'Varsha', 'Riya', 'Sandeep', 'James', 'Amir', 'Leena', 'Priya', 'Tom', 'Hannah']\n",
        "last_names  = ['Patel', 'Khan', 'Smith', 'Williams', 'Brown', 'Shah', 'Singh', 'Jones', 'Taylor', 'Ahmed']\n",
        "\n",
        "names = [\n",
        "    f\"{np.random.choice(first_names)} {np.random.choice(last_names)}\"\n",
        "    for _ in range(num_users)\n",
        "]"
      ],
      "metadata": {
        "id": "Q56_8o6ygfVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users = pd.DataFrame({\n",
        "    'user_id': user_ids,\n",
        "    'name': names,\n",
        "    'phone': phone_numbers,\n",
        "    'pincode': pincodes,\n",
        "    'signup_date': signup_dates,\n",
        "    'age': ages,\n",
        "    'device': devices\n",
        "})"
      ],
      "metadata": {
        "id": "sVOa8RQegp7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(users.head())\n",
        "print(users.info())"
      ],
      "metadata": {
        "id": "q6yXyKHiguUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users.shape"
      ],
      "metadata": {
        "id": "Q_ogHFqdg7qC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users.device.value_counts().head()"
      ],
      "metadata": {
        "id": "4CVYqcd5hFpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users.sample(1)"
      ],
      "metadata": {
        "id": "MDzcjhrchPTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRANSACTIONS"
      ],
      "metadata": {
        "id": "0SGGRBIYjR_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users.to_csv(\"users.csv\", index=False)"
      ],
      "metadata": {
        "id": "_wrKdjVAlo2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir()"
      ],
      "metadata": {
        "id": "Lkh-pEm4Yv1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "\n",
        "BASE_DIR = '/content/drive/MyDrive/monzo_user_analytics'  # change if you like\n",
        "import os\n",
        "os.makedirs(f'{BASE_DIR}/data', exist_ok=True)\n",
        "\n",
        "# Example saves:\n",
        "users.to_csv(f'{BASE_DIR}/data/users.csv', index=False)\n",
        "#transactions.to_csv(f'{BASE_DIR}/data/transactions.csv', index=False)"
      ],
      "metadata": {
        "id": "sxmMtro2hTLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "check = pd.read_csv(f'{BASE_DIR}/data/users.csv', parse_dates=['signup_date'])\n",
        "check.head()"
      ],
      "metadata": {
        "id": "DPMyAr1YjTMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# Paths (we’ll keep the same BASE_DIR you already set earlier)\n",
        "TXNS_PATH = f\"{BASE_DIR}/data/transactions.csv\"\n",
        "os.makedirs(f\"{BASE_DIR}/data\", exist_ok=True)\n",
        "\n",
        "# --- STEP 1: Read users again to access their signup dates ---\n",
        "users = pd.read_csv(f\"{BASE_DIR}/data/users.csv\", parse_dates=[\"signup_date\"])\n",
        "user_ids = users[\"user_id\"].to_numpy()\n",
        "signups = users[\"signup_date\"].to_numpy(\"datetime64[D]\")\n",
        "n_users = len(user_ids)\n",
        "\n",
        "rng = np.random.default_rng(42)  # for reproducibility\n",
        "\n",
        "# --- STEP 2: Decide how many transactions per user (light, medium, heavy) ---\n",
        "u = rng.random(n_users)\n",
        "txn_counts = np.empty(n_users, dtype=np.int32)\n",
        "txn_counts[u < 0.6] = rng.integers(1, 21, size=(u < 0.6).sum())      # light\n",
        "txn_counts[(u >= 0.6) & (u < 0.9)] = rng.integers(20, 81, size=((u >= 0.6) & (u < 0.9)).sum())  # medium\n",
        "txn_counts[u >= 0.9] = rng.integers(80, 201, size=(u >= 0.9).sum())  # heavy\n",
        "\n",
        "total_txns = int(txn_counts.sum())\n",
        "print(f\"Total transactions to generate: {total_txns:,}\")\n",
        "\n",
        "# --- STEP 3: Repeat user info per transaction (vectorized, no loops) ---\n",
        "rep_user_ids = np.repeat(user_ids, txn_counts)\n",
        "rep_signups  = np.repeat(signups,  txn_counts)\n",
        "\n",
        "# --- STEP 4: Generate transaction fields ---\n",
        "day_offsets = rng.integers(0, 366, size=total_txns)        # days after signup\n",
        "txn_dates   = rep_signups + day_offsets.astype(\"timedelta64[D]\")\n",
        "txn_dates   = np.minimum(txn_dates, np.datetime64(\"2024-12-31\"))\n",
        "\n",
        "amounts = np.round(rng.exponential(scale=35, size=total_txns) + 1.0, 2)  # skewed realistic spends\n",
        "categories = np.array([\"groceries\", \"transport\", \"bills\", \"shopping\", \"restaurants\", \"entertainment\"])\n",
        "channels   = np.array([\"card\", \"online\", \"direct_debit\"])\n",
        "txn_cats   = categories[rng.integers(0, len(categories), size=total_txns)]\n",
        "txn_chans  = channels[rng.integers(0, len(channels),   size=total_txns)]\n",
        "\n",
        "# --- STEP 5: Build DataFrame ---\n",
        "transactions = pd.DataFrame({\n",
        "    \"user_id\": rep_user_ids,\n",
        "    \"txn_date\": pd.to_datetime(txn_dates),\n",
        "    \"amount\": amounts,\n",
        "    \"category\": txn_cats,\n",
        "    \"channel\": txn_chans,\n",
        "})\n",
        "\n",
        "transactions.sort_values([\"user_id\", \"txn_date\"], inplace=True, kind=\"mergesort\")\n",
        "transactions.reset_index(drop=True, inplace=True)\n",
        "transactions.insert(0, \"txn_id\", np.arange(1, len(transactions) + 1, dtype=np.int64))\n",
        "\n",
        "# --- STEP 6: Save & verify ---\n",
        "transactions.to_csv(TXNS_PATH, index=False)\n",
        "print(f\"\\nSaved transactions to: {TXNS_PATH}\")\n",
        "\n",
        "# --- STEP 7: Quick checks ---\n",
        "print(\"\\n--- Sanity checks ---\")\n",
        "print(\"shape:\", transactions.shape)\n",
        "print(\"date range:\", transactions[\"txn_date\"].min(), \"→\", transactions[\"txn_date\"].max())\n",
        "print(\"unique users:\", transactions[\"user_id\"].nunique())\n",
        "print(\"\\namounts:\\n\", transactions[\"amount\"].describe())\n",
        "print(\"\\ntransactions per user:\\n\", transactions.groupby(\"user_id\").size().describe())"
      ],
      "metadata": {
        "id": "09od0NJ2lyDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Paths\n",
        "FEATURES_PATH = f\"{BASE_DIR}/data/feature_usage.csv\"\n",
        "USERS_PATH    = f\"{BASE_DIR}/data/users.csv\"\n",
        "\n",
        "os.makedirs(f\"{BASE_DIR}/data\", exist_ok=True)\n",
        "\n",
        "# Load users (need user_id, signup_date, device, age)\n",
        "users = pd.read_csv(USERS_PATH, parse_dates=[\"signup_date\"])\n",
        "user_ids = users[\"user_id\"].to_numpy()\n",
        "signups  = users[\"signup_date\"].to_numpy(\"datetime64[D]\")\n",
        "devices  = users[\"device\"].to_numpy()\n",
        "ages     = users[\"age\"].to_numpy()\n",
        "n_users  = len(users)\n",
        "\n",
        "rng = np.random.default_rng(123)\n",
        "\n",
        "# Define features\n",
        "features = np.array([\"pots\", \"get_paid_early\", \"budgeting\", \"roundups\", \"investments\", \"pensions\"])\n",
        "\n",
        "# --- 1) Per-user adoption probabilities (simple, interpretable rules) ---\n",
        "# Start from a base probability per feature\n",
        "base_p = {\n",
        "    \"pots\":            0.45,\n",
        "    \"get_paid_early\":  0.35,\n",
        "    \"budgeting\":       0.40,\n",
        "    \"roundups\":        0.30,\n",
        "    \"investments\":     0.15,\n",
        "    \"pensions\":        0.10,\n",
        "}\n",
        "\n",
        "# Device adjustments (e.g., iOS users slightly more likely to adopt budgeting; Web less)\n",
        "device_adj = np.zeros(n_users)\n",
        "device_adj[devices == \"iOS\"]     =  +0.05\n",
        "device_adj[devices == \"Android\"] =  +0.00\n",
        "device_adj[devices == \"Web\"]     =  -0.05\n",
        "\n",
        "# Age adjustments (older slightly more likely to adopt pensions/investments)\n",
        "age_adj_invest   = np.clip((ages - 25) / 50, 0, 0.10)   # up to +0.10\n",
        "age_adj_pensions = np.clip((ages - 30) / 40, 0, 0.12)   # up to +0.12\n",
        "\n",
        "# Build a (n_users x n_features) matrix of adoption probabilities\n",
        "P = np.zeros((n_users, len(features)), dtype=float)\n",
        "for j, f in enumerate(features):\n",
        "    p = base_p[f] + device_adj\n",
        "    if f == \"investments\":\n",
        "        p = p + age_adj_invest\n",
        "    if f == \"pensions\":\n",
        "        p = p + age_adj_pensions\n",
        "    P[:, j] = np.clip(p, 0.01, 0.95)  # ensure valid prob bounds\n",
        "\n",
        "# --- 2) Sample which features each user adopts (Bernoulli trials) ---\n",
        "adopt_matrix = rng.random(P.shape) < P   # True if adopted\n",
        "adopt_counts = adopt_matrix.sum(axis=1)  # features adopted per user (for sanity)\n",
        "\n",
        "# --- 3) For each adopted feature, generate a number of usage events ---\n",
        "# Usage intensity by feature (mean events per adopted feature)\n",
        "feature_mean_uses = {\n",
        "    \"pots\":            8,\n",
        "    \"get_paid_early\":  6,\n",
        "    \"budgeting\":       10,\n",
        "    \"roundups\":        20,\n",
        "    \"investments\":     4,\n",
        "    \"pensions\":        2,\n",
        "}\n",
        "\n",
        "# Build counts of usage events per (user, feature)\n",
        "usage_rows_user   = []\n",
        "usage_rows_feat   = []\n",
        "usage_rows_date   = []\n",
        "usage_rows_action = []  # \"enable\" or \"use\"\n",
        "\n",
        "# Precompute mean array aligned with features\n",
        "means = np.array([feature_mean_uses[f] for f in features], dtype=float)\n",
        "\n",
        "# We vectorize by:\n",
        "# 1) get indices where feature is adopted\n",
        "u_idx, f_idx = np.where(adopt_matrix)  # arrays of same length\n",
        "n_adopt_events = u_idx.size\n",
        "\n",
        "# 2) Sample #uses per adopted feature via Poisson (>=1), clipped\n",
        "uses = rng.poisson(lam=means[f_idx]) + 1\n",
        "uses = np.clip(uses, 1, 60)\n",
        "\n",
        "# 3) For each (user, feature), create 1 \"enable\" event near signup + N \"use\" events spread across 2024\n",
        "# We'll do this in chunks to avoid huge Python loops; but we still need to expand variable lengths.\n",
        "# Chunk size controls memory usage; adjust if needed.\n",
        "chunk = 200_000\n",
        "for start in range(0, n_adopt_events, chunk):\n",
        "    end = min(start + chunk, n_adopt_events)\n",
        "\n",
        "    u_block = u_idx[start:end]\n",
        "    f_block = f_idx[start:end]\n",
        "    k_block = uses[start:end]\n",
        "\n",
        "    # ENABLE events (one per adopted feature)\n",
        "    enable_dates = signups[u_block] + rng.integers(0, 30, size=len(u_block)).astype(\"timedelta64[D]\")\n",
        "    enable_dates = np.minimum(enable_dates, np.datetime64(\"2024-12-31\"))\n",
        "\n",
        "    usage_rows_user.extend(user_ids[u_block])\n",
        "    usage_rows_feat.extend(features[f_block])\n",
        "    usage_rows_date.extend(enable_dates.astype(\"datetime64[ns]\"))\n",
        "    usage_rows_action.extend([\"enable\"] * len(u_block))\n",
        "\n",
        "    # USE events (variable counts) -> expand each row by k_block\n",
        "    rep_users = np.repeat(user_ids[u_block], k_block)\n",
        "    rep_feats = np.repeat(features[f_block], k_block)\n",
        "    rep_signup = np.repeat(signups[u_block], k_block)\n",
        "\n",
        "    day_offsets = rng.integers(0, 366, size=rep_users.size)\n",
        "    use_dates = rep_signup + day_offsets.astype(\"timedelta64[D]\")\n",
        "    use_dates = np.minimum(use_dates, np.datetime64(\"2024-12-31\"))\n",
        "\n",
        "    usage_rows_user.extend(rep_users)\n",
        "    usage_rows_feat.extend(rep_feats)\n",
        "    usage_rows_date.extend(use_dates.astype(\"datetime64[ns]\"))\n",
        "    usage_rows_action.extend([\"use\"] * rep_users.size)\n",
        "\n",
        "# --- 4) Assemble DataFrame ---\n",
        "feature_usage = pd.DataFrame({\n",
        "    \"user_id\": usage_rows_user,\n",
        "    \"feature\": usage_rows_feat,\n",
        "    \"event_date\": pd.to_datetime(usage_rows_date),\n",
        "    \"action\": usage_rows_action,\n",
        "})\n",
        "\n",
        "feature_usage.sort_values([\"user_id\", \"event_date\", \"feature\", \"action\"], inplace=True, kind=\"mergesort\")\n",
        "feature_usage.reset_index(drop=True, inplace=True)\n",
        "feature_usage.insert(0, \"event_id\", np.arange(1, len(feature_usage) + 1, dtype=np.int64))\n",
        "\n",
        "# --- 5) Save & sanity checks ---\n",
        "feature_usage.to_csv(FEATURES_PATH, index=False)\n",
        "print(f\"Saved: {FEATURES_PATH}\")\n",
        "\n",
        "print(\"\\n--- Sanity checks ---\")\n",
        "print(\"shape:\", feature_usage.shape)\n",
        "print(\"date range:\", feature_usage[\"event_date\"].min(), \"→\", feature_usage[\"event_date\"].max())\n",
        "print(\"unique users in features:\", feature_usage[\"user_id\"].nunique())\n",
        "print(\"\\nCounts by action:\\n\", feature_usage[\"action\"].value_counts())\n",
        "print(\"\\nTop features by events:\\n\", feature_usage[\"feature\"].value_counts().head(10))\n",
        "print(\"\\nAdopted features per user (mean):\", adopt_counts.mean().round(2))"
      ],
      "metadata": {
        "id": "QiaDEAdWn4wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Monthly Active Users (MAU)"
      ],
      "metadata": {
        "id": "-gwqQ85koaxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tx = pd.read_csv(f\"{BASE_DIR}/data/transactions.csv\", parse_dates=[\"txn_date\"])\n",
        "mau = (tx\n",
        "       .assign(month=tx[\"txn_date\"].dt.to_period(\"M\"))\n",
        "       .groupby(\"month\")[\"user_id\"].nunique()\n",
        "       .rename(\"MAU\")\n",
        "       .to_frame()\n",
        "       .reset_index())\n",
        "mau[\"month\"] = mau[\"month\"].astype(str)\n",
        "mau.head(), mau.tail()"
      ],
      "metadata": {
        "id": "nhvTrCaRoHux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature adoption rate (what % of users enabled each feature)"
      ],
      "metadata": {
        "id": "NJ0_rhlXofwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fu = pd.read_csv(f\"{BASE_DIR}/data/feature_usage.csv\", parse_dates=[\"event_date\"])\n",
        "adoption = (fu[fu[\"action\"]==\"enable\"]\n",
        "            .groupby(\"feature\")[\"user_id\"].nunique()\n",
        "            .rename(\"adopters\")\n",
        "            .to_frame()\n",
        "            .reset_index())\n",
        "adoption[\"adoption_rate\"] = adoption[\"adopters\"] / users.shape[0]\n",
        "adoption.sort_values(\"adoption_rate\", ascending=False)"
      ],
      "metadata": {
        "id": "UMhHIAV5oQhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spend by device (are iOS users spending more?)"
      ],
      "metadata": {
        "id": "THZtKmLTojTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tx_users = tx.merge(users[[\"user_id\",\"device\"]], on=\"user_id\", how=\"left\")\n",
        "spend_by_device = (tx_users\n",
        "                   .groupby(\"device\")[\"amount\"]\n",
        "                   .agg([\"count\",\"mean\",\"sum\"])\n",
        "                   .rename(columns={\"count\":\"txn_cnt\",\"mean\":\"avg_txn\",\"sum\":\"total_spend\"})\n",
        "                   .reset_index())\n",
        "spend_by_device"
      ],
      "metadata": {
        "id": "mYIuqLZgoSbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "New vs old users"
      ],
      "metadata": {
        "id": "bpOVJn0MopCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users[\"signup_month\"] = users[\"signup_date\"].dt.to_period(\"M\").astype(str)\n",
        "tx_users = tx.merge(users[[\"user_id\",\"signup_month\"]], on=\"user_id\", how=\"left\")\n",
        "spend_by_signup_month = (tx_users\n",
        "                         .assign(tx_month=tx_users[\"txn_date\"].dt.to_period(\"M\").astype(str))\n",
        "                         .groupby([\"signup_month\",\"tx_month\"])[\"user_id\"]\n",
        "                         .nunique()\n",
        "                         .rename(\"active_users\")\n",
        "                         .reset_index())\n",
        "spend_by_signup_month.head()"
      ],
      "metadata": {
        "id": "3wnLdiDPorgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert your earlier MAU calculation to a proper plot\n",
        "plt.figure(figsize=(8,5), dpi=120)\n",
        "plt.plot(mau[\"month\"], mau[\"MAU\"], marker='o', color='purple')\n",
        "plt.title(\"Monthly Active Users (MAU) Over Time\")\n",
        "plt.ylabel(\"Active Users\")\n",
        "plt.xlabel(\"Month (2024)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TAqd7sPIqITF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cohort = spend_by_signup_month[spend_by_signup_month[\"signup_month\"] == \"2024-01\"]\n",
        "\n",
        "plt.figure(figsize=(8,5), dpi=120)\n",
        "plt.plot(cohort[\"tx_month\"], cohort[\"active_users\"], marker='o', color='teal')\n",
        "plt.title(\"Cohort Retention: Users Signed Up in Jan 2024\")\n",
        "plt.ylabel(\"Active Users (same cohort)\")\n",
        "plt.xlabel(\"Month of Activity\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qUI_ac1uqcor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "BASE_DIR = '/content/drive/MyDrive/banking-user-analysis'\n",
        "import os; os.makedirs(f'{BASE_DIR}/data', exist_ok=True)\n",
        "os.makedirs(f'{BASE_DIR}/images', exist_ok=True)\n",
        "os.makedirs(f'{BASE_DIR}/notebooks', exist_ok=True)"
      ],
      "metadata": {
        "id": "ttueNNN0AwvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users.to_csv(f'{BASE_DIR}/data/users.csv', index=False)\n",
        "transactions.to_csv(f'{BASE_DIR}/data/transactions.csv', index=False)\n",
        "feature_usage.to_csv(f'{BASE_DIR}/data/feature_usage.csv', index=False)"
      ],
      "metadata": {
        "id": "u3RQijFGA7no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.savefig(f'{BASE_DIR}/images/feature_adoption.png', bbox_inches='tight')\n",
        "# do similar for other charts:\n",
        "# spend_by_device.png, mau_trend.png, cohort_retention.png"
      ],
      "metadata": {
        "id": "jH0CAYfJBne5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "readme = \"\"\"<paste the README content here>\"\"\"\n",
        "with open(f'{BASE_DIR}/README.md','w') as f:\n",
        "    f.write(readme)"
      ],
      "metadata": {
        "id": "mTmKBUw6BrpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "tx_small = transactions.sample(n=250000, random_state=42)  # tweak n to keep <100MB\n",
        "tx_small.to_csv(f'{BASE_DIR}/data/transactions_sample.csv', index=False)"
      ],
      "metadata": {
        "id": "uVxSA8jIBxlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transactions['month'] = transactions['txn_date'].dt.to_period('M').astype(str)\n",
        "for m, dfm in transactions.groupby('month'):\n",
        "    dfm.drop(columns='month').to_csv(f'{BASE_DIR}/data/transactions_{m}.csv', index=False)"
      ],
      "metadata": {
        "id": "5Q47L_X6B0oC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "to accomodate github commit"
      ],
      "metadata": {
        "id": "kmELU2HXGSiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# load your originals from Drive\n",
        "users = pd.read_csv(f'{BASE_DIR}/data/users.csv')\n",
        "transactions = pd.read_csv(f'{BASE_DIR}/data/transactions.csv')\n",
        "feature_usage = pd.read_csv(f'{BASE_DIR}/data/feature_usage.csv')\n",
        "\n",
        "# create smaller versions (adjust n as needed)\n",
        "users_sample = users.sample(n=2000, random_state=42)\n",
        "transactions_sample = transactions.sample(n=50000, random_state=42)\n",
        "feature_usage_sample = feature_usage.sample(n=100000, random_state=42)\n",
        "\n",
        "# save them with _sample suffix\n",
        "users_sample.to_csv(f'{BASE_DIR}/data/users_sample.csv', index=False)\n",
        "transactions_sample.to_csv(f'{BASE_DIR}/data/transactions_sample.csv', index=False)\n",
        "feature_usage_sample.to_csv(f'{BASE_DIR}/data/feature_usage_sample.csv', index=False)"
      ],
      "metadata": {
        "id": "CsFyHukwGR2h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}